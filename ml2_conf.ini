[ml2]
tenant_network_types = vxlan,gre
type_drivers = vxlan,gre,svlan,flat,local,vlan
mechanism_drivers = openvswitch,svlan,l2population
# (ListOpt) List of network type driver entrypoints to be loaded from
# the neutron.ml2.type_drivers namespace.
#
# type_drivers = local,flat,vlan,gre,vxlan
# Example: type_drivers = flat,vlan,gre,vxlan

# (ListOpt) Ordered list of network_types to allocate as tenant
# networks. The default value 'local' is useful for single-box testing
# but provides no connectivity between hosts.
#
# tenant_network_types = local
# Example: tenant_network_types = vlan,gre,vxlan

# (ListOpt) Ordered list of networking mechanism driver entrypoints
# to be loaded from the neutron.ml2.mechanism_drivers namespace.
# mechanism_drivers =
# Example: mechanism_drivers = arista
# Example: mechanism_drivers = cisco,logger

# =========== items for MTU selection and advertisement =============
# (IntOpt) Path MTU.  The maximum permissible size of an unfragmented
# packet travelling from and to addresses where encapsulated Neutron
# traffic is sent.  Drivers calculate maximum viable MTU for
# validating tenant requests based on this value (typically,
# path_mtu - max encap header size).  If <=0, the path MTU is
# indeterminate and no calculation takes place.
# path_mtu = 0

# (IntOpt) Segment MTU.  The maximum permissible size of an
# unfragmented packet travelling a L2 network segment.  If <=0,
# the segment MTU is indeterminate and no calculation takes place.
# segment_mtu = 0

# (ListOpt) Physical network MTUs.  List of mappings of physical
# network to MTU value.  The format of the mapping is
# <physnet>:<mtu val>.  This mapping allows specifying a
# physical network MTU value that differs from the default
# segment_mtu value.
# physical_network_mtus =
# Example: physical_network_mtus = physnet1:1550, physnet2:1500
# ======== end of items for MTU selection and advertisement =========

[ml2_type_flat]
# (ListOpt) List of physical_network names with which flat networks
# can be created. Use * to allow flat networks with arbitrary
# physical_network names.
#
# flat_networks =
# Example:flat_networks = physnet1,physnet2
# Example:flat_networks = *

[ml2_type_vlan]
# (ListOpt) List of <physical_network>[:<vlan_min>:<vlan_max>] tuples
# specifying physical_network names usable for VLAN provider and
# tenant networks, as well as ranges of VLAN tags on each
# physical_network available for allocation as tenant networks.
#
# network_vlan_ranges =
# Example: network_vlan_ranges = physnet1:1000:2999,physnet2

[ml2_type_gre]
tunnel_id_ranges = 50:100
# (ListOpt) Comma-separated list of <tun_min>:<tun_max> tuples enumerating ranges of GRE tunnel IDs that are available for tenant network allocation
# tunnel_id_ranges =

[ml2_type_vxlan]
vni_ranges = 400:500
# (ListOpt) Comma-separated list of <vni_min>:<vni_max> tuples enumerating
# ranges of VXLAN VNI IDs that are available for tenant network allocation.
#
# vni_ranges =

# (StrOpt) Multicast group for the VXLAN interface. When configured, will
# enable sending all broadcast traffic to this multicast group. When left
# unconfigured, will disable multicast VXLAN mode.
#
# vxlan_group =
# Example: vxlan_group = 239.1.1.1

[ovs]
enable_tunneling = True
local_ip = 10.180.156.19
# Enable accesslist control, only available for SVLan network type
enable_acl = True

# Whether allow arp broadcast passthrough, for SVLan network type
permit_arp_broadcast = True

#Enable layer2 security for ARP spoofing, illegal MAC/IP
#and DHCP server packets
enable_l2_security = True

[QOS]
#default is true, and only for egress traffic from VM
qos_enabled = true
#which physical device will be configured for QOS, can be more than one
#device, separated by comma, exp: qos_phy_dev=eth0, eth1
qos_phy_dev = eth0
#unit:Mbps. The rule is like this:
#packet rate < dev_prefer_rate, high priority
#packet rate> dev_perfer_rate and < dev_max_rate, low priority
#packet rate > dev_max_rate, drop
#dev_max_rate = 1000
#dev_prefer_rate = 20

#burst can be maually set or automatically calculated
#if not set or set to 0, burst will be auto calculated
#unit:Kbit(Mbit*1024)
#dev_max_burst = 76800
#dev_prefer_burst = 1024

#unit:Kpps
#dev_max_pps is used to derive mpu value for VM ingress qdisc, according to
#the value observed by linux networking stack performance, 50kpps is a proper
#value and should be used if want to derive mpu.
#The default value is 50kpps which means enable mpu automatically if l2 qos
#is enabled.
#dev_max_pps = 50
[database]
connection = mysql://root:ntse@localhost/neutron_ml2?charset=utf8

[SECURITYGROUP]
firewall_driver = neutron.agent.not.a.real.FirewallDriver

[agent]
tunnel_types = vxlan,gre
minimize_polling = True
l2_population = True
root_helper = sudo /usr/local/bin/neutron-rootwrap /etc/neutron/rootwrap.conf

[securitygroup]
firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver
